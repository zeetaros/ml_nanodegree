{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from factory_func import plot_confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, Lambda, LSTM, ConvLSTM2D, TimeDistributed, Masking, Bidirectional\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate, Activation, MaxPooling1D, GlobalAveragePooling1D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Analysis  \n",
    "### Accuracy Records  \n",
    "SVM: 45.5172%  \n",
    "Dense NN: 45.1724%  \n",
    "CNN: 47.2414%  \n",
    "LSTM: tbc  \n",
    "  \n",
    "*(there might be a ceiling for how much you can improve the accuracy)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_raw = pd.read_csv(os.path.join(os.getcwd(), 'dev_sent_emo.csv'))\n",
    "train_raw = pd.read_csv(os.path.join(os.getcwd(), 'train_sent_emo.csv'))\n",
    "test_raw = pd.read_csv(os.path.join(os.getcwd(), 'test_sent_emo.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh my God, hes lost it. Hes totally lost it.</td>\n",
       "      <td>Phoebe</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:20:57,256</td>\n",
       "      <td>00:21:00,049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What?</td>\n",
       "      <td>Monica</td>\n",
       "      <td>surprise</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:21:01,927</td>\n",
       "      <td>00:21:03,261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Or! Or, we could go to the bank, close our acc...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:24,660</td>\n",
       "      <td>00:12:30,915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Youre a genius!</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:32,334</td>\n",
       "      <td>00:12:33,960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Aww, man, now we wont be bank buddies!</td>\n",
       "      <td>Joey</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:34,211</td>\n",
       "      <td>00:12:37,505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance   Speaker  \\\n",
       "0       1     Oh my God, hes lost it. Hes totally lost it.    Phoebe   \n",
       "1       2                                              What?    Monica   \n",
       "2       3  Or! Or, we could go to the bank, close our acc...      Ross   \n",
       "3       4                                   Youre a genius!  Chandler   \n",
       "4       5            Aww, man, now we wont be bank buddies!      Joey   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   sadness  negative            0             0       4        7   \n",
       "1  surprise  negative            0             1       4        7   \n",
       "2   neutral   neutral            1             0       4        4   \n",
       "3       joy  positive            1             1       4        4   \n",
       "4   sadness  negative            1             2       4        4   \n",
       "\n",
       "      StartTime       EndTime  \n",
       "0  00:20:57,256  00:21:00,049  \n",
       "1  00:21:01,927  00:21:03,261  \n",
       "2  00:12:24,660  00:12:30,915  \n",
       "3  00:12:32,334  00:12:33,960  \n",
       "4  00:12:34,211  00:12:37,505  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_raw.Utterance = dev_raw.Utterance.apply(lambda x: re.sub('\\\\x92', \"'\", x))\n",
    "train_raw.Utterance = train_raw.Utterance.apply(lambda x: re.sub('\\\\x92', \"'\", x))\n",
    "test_raw.Utterance = test_raw.Utterance.apply(lambda x: re.sub('\\\\x92', \"'\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = dev_raw.Utterance\n",
    "y_dev = dev_raw.Emotion\n",
    "x_train = train_raw.Utterance\n",
    "y_train = train_raw.Emotion\n",
    "x_test = test_raw.Utterance\n",
    "y_test = test_raw.Emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1,3))\n",
    "tfidf.fit(x_train)\n",
    "x_dev_tf = tfidf.transform(x_dev)\n",
    "x_train_tf = tfidf.transform(x_train)\n",
    "x_test_tf = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=300)\n",
    "# x_dev_tr = svd.fit_transform(x_dev_tf)\n",
    "# x_train_tr = svd.fit_transform(x_train_tf)\n",
    "# x_test_tr = svd.fit_transform(x_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=10, kernel='linear', probability=True)\n",
    "# param_grid = { \n",
    "#     'C': [1,10,100], 'kernel': ['linear', 'rbf']\n",
    "# }\n",
    "# clf = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5)\n",
    "# clf.fit(x_train_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model by GridSearchCV**  \n",
    "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,  \n",
    "decision_function_shape='ovr', degree=3, gamma='auto_deprecated',  \n",
    "kernel='linear', max_iter=-1, probability=True, random_state=None,  \n",
    "shrinking=True, tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X=x_train_tf, y=y_train)\n",
    "y_pred_svm = svm.predict(x_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42923347861223476"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=y_pred_svm, y_true=y_test, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 45.5172%\n"
     ]
    }
   ],
   "source": [
    "svm_accuracy = 100*np.sum(y_pred_svm==y_test)/len(y_pred_svm)\n",
    "print('Test accuracy: %.4f%%' % svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras TF-IDF tokenizer + Neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network won't accept sentences with different dimension(i.e. number of words) as input. By padding the inputs, we decide the maximum length of words in a sentence, then zero pads the rest, if the input length is shorter than the designated length. In the case where it exceeds the maximum length, then it will also truncate either from the beginning or from the end.  \n",
    "*Ref_1* https://keras.io/preprocessing/sequence/  \n",
    "*Ref_2* https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74\n",
    "\n",
    "https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n",
    "#### Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = dev_raw.Utterance\n",
    "y_dev = dev_raw.Emotion\n",
    "x_train = train_raw.Utterance\n",
    "y_train = train_raw.Emotion\n",
    "x_test = test_raw.Utterance\n",
    "y_test = test_raw.Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "# sequences = tokenizer.texts_to_sequences(x_train)\n",
    "# data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'joy': 3,\n",
       " 'neutral': 4,\n",
       " 'sadness': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "targets, uniques = pd.factorize(y_train, sort=True)\n",
    "y_train = to_categorical(targets, num_classes)\n",
    "\n",
    "label_map = dict(zip(list(uniques), range(num_classes)))\n",
    "label_map\n",
    "# len(y_true) == len(y_pred_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.fit_on_sequences(x_dev)\n",
    "# tokenizer.texts_to_sequences(x_dev)\n",
    "# tokenizer.texts_to_matrix(x_dev, mode='tfidf')\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_transf = list(map(label_map.get, y_test))\n",
    "y_true = to_categorical(np.array(y_transf), num_classes)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_checkpts = ModelCheckpoint(filepath=os.path.join(os.getcwd() + '/saved_models/dense.weights.best.hdf5'), \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 32)                640032    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 651,367\n",
      "Trainable params: 651,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 7\n",
    "\n",
    "dense = Sequential()\n",
    "dense.add(Dense(32, activation='relu', input_dim=20000))\n",
    "# model.add(Dropout(0.2))\n",
    "dense.add(Dense(64, activation='relu'))\n",
    "dense.add(Dense(128, activation='relu'))\n",
    "dense.add(Dense(num_classes, activation='softmax'))\n",
    "dense.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "dense.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D is generally good for text, whereas Conv2D is good for audio and images where spatial matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5993 samples, validate on 3996 samples\n",
      "Epoch 1/10\n",
      "5993/5993 [==============================] - 6s 1ms/step - loss: 1.5412 - acc: 0.4762 - val_loss: 1.4232 - val_acc: 0.5083\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.42323, saving model to C:\\Users\\syip\\Desktop\\DaSci\\Nanodegree\\ml_nanodegree\\capstone/saved_models/dense.weights.best.hdf5\n",
      "Epoch 2/10\n",
      "5993/5993 [==============================] - 4s 727us/step - loss: 1.1340 - acc: 0.5972 - val_loss: 1.5039 - val_acc: 0.5053\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.42323\n",
      "Epoch 3/10\n",
      "5993/5993 [==============================] - 4s 699us/step - loss: 0.7546 - acc: 0.7402 - val_loss: 1.7573 - val_acc: 0.4479\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.42323\n",
      "Epoch 4/10\n",
      "5993/5993 [==============================] - 4s 723us/step - loss: 0.5112 - acc: 0.8241 - val_loss: 2.0672 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.42323\n",
      "Epoch 5/10\n",
      "5993/5993 [==============================] - 4s 726us/step - loss: 0.3853 - acc: 0.8690 - val_loss: 2.3884 - val_acc: 0.4605\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.42323\n",
      "Epoch 6/10\n",
      "5993/5993 [==============================] - 5s 785us/step - loss: 0.3147 - acc: 0.8935 - val_loss: 2.5746 - val_acc: 0.4515\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.42323\n",
      "Epoch 7/10\n",
      "5993/5993 [==============================] - 4s 734us/step - loss: 0.2704 - acc: 0.9087 - val_loss: 2.8275 - val_acc: 0.4499\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.42323\n",
      "Epoch 8/10\n",
      "5993/5993 [==============================] - 4s 678us/step - loss: 0.2414 - acc: 0.9172 - val_loss: 3.0937 - val_acc: 0.4332\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.42323\n",
      "Epoch 9/10\n",
      "5993/5993 [==============================] - 5s 805us/step - loss: 0.2216 - acc: 0.9221 - val_loss: 3.2223 - val_acc: 0.4492\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.42323\n",
      "Epoch 10/10\n",
      "5993/5993 [==============================] - 6s 1ms/step - loss: 0.2096 - acc: 0.9261 - val_loss: 3.3708 - val_acc: 0.4532\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.42323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa0c34d438>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.fit(x_train, y_train, validation_split=0.4, epochs=10, callbacks=[dense_checkpts], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=50)\n",
    "y_pred_dense = dense.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 1606, 3: 300, 0: 188, 5: 156, 6: 289, 1: 29, 2: 42})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_pred_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 45.1724%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 100*np.sum(y_pred_dense==y_true)/len(y_true)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'], dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__About batch_size__  \n",
    "Advantages of using a batch size < number of all samples:\n",
    "\n",
    "- It requires less memory. Since you train the network using fewer samples, the overall training procedure requires less memory. That's especially important if you are not able to fit the whole dataset in your machine's memory.\n",
    "\n",
    "- Typically networks train faster with mini-batches. That's because we update the weights after each propagation. In our example we've propagated 11 batches (10 of them had 100 samples and 1 had 50 samples) and after each of them we've updated our network's parameters. If we used all samples during propagation we would make only 1 update for the network's parameter.\n",
    "\n",
    "Disadvantages of using a batch size < number of all samples:\n",
    "\n",
    "- The smaller the batch the less accurate the estimate of the gradient will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "n_length = x_train.shape[0]\n",
    "n_features = x_train.shape[1]\n",
    "\n",
    "x_train_reshaped = x_train.reshape(n_length, n_features, 1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], n_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_checkpts = ModelCheckpoint(filepath=os.path.join(os.getcwd() + '/saved_models/cnn.weights.best.hdf5'), \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 20000, 16)         48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 10000, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 10000, 32)         1056      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 5000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 5000, 64)          4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 2500, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160000)            0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               20480128  \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 20,486,295\n",
      "Trainable params: 20,486,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(n_features, 1)))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "cnn.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "cnn.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "# cnn.add(GlobalAveragePooling1D())\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "cnn.add(Dense(num_classes, activation='softmax'))\n",
    "cnn.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5993 samples, validate on 3996 samples\n",
      "Epoch 1/5\n",
      "5993/5993 [==============================] - 152s 25ms/step - loss: 1.5315 - acc: 0.4721 - val_loss: 1.4881 - val_acc: 0.4817\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.48806, saving model to C:\\Users\\syip\\Desktop\\DaSci\\Nanodegree\\ml_nanodegree\\capstone/saved_models/cnn.weights.best.hdf5\n",
      "Epoch 2/5\n",
      "5993/5993 [==============================] - 151s 25ms/step - loss: 1.2260 - acc: 0.5493 - val_loss: 1.4904 - val_acc: 0.4652\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.48806\n",
      "Epoch 3/5\n",
      "5993/5993 [==============================] - 149s 25ms/step - loss: 0.8607 - acc: 0.6843 - val_loss: 1.7995 - val_acc: 0.4637\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.48806\n",
      "Epoch 4/5\n",
      "5993/5993 [==============================] - 148s 25ms/step - loss: 0.5945 - acc: 0.7829 - val_loss: 2.2183 - val_acc: 0.4695\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.48806\n",
      "Epoch 5/5\n",
      "5993/5993 [==============================] - 150s 25ms/step - loss: 0.4472 - acc: 0.8398 - val_loss: 2.6401 - val_acc: 0.4630\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.48806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f2011e1cf8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x_train_reshaped, y_train, validation_split=0.4, epochs=5, callbacks=[cnn_checkpts], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 44.0613%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 1728, 0: 198, 3: 256, 5: 118, 1: 40, 6: 249, 2: 21})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_cnn = cnn.predict_classes(x_test_reshaped)\n",
    "# y_pred_cnn = [np.argmax(x) for x in cnn.predict(x_test_reshaped)]\n",
    "Counter(y_pred_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 47.2414%\n"
     ]
    }
   ],
   "source": [
    "cnn_accuracy = 100*np.sum(y_pred_cnn==y_true)/len(y_pred_cnn)\n",
    "print('Test accuracy: %.4f%%' % cnn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_score = cnn.evaluate(x_test_reshaped, y_pred_cnn, verbose=1)\n",
    "# print(\"%s: %.2f%%\" % (cnn.metrics_names[1], cnn_score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialise the model architecture to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.getcwd(), \"saved_models\", \"cnn.model.json\", \"w\") as json_file:\n",
    "    json_file.write(cnn.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_cnn_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_cnn = model_from_json(loaded_cnn_json)\n",
    "# load weights into new model\n",
    "loaded_cnn.load_weights(\"cnn.weights.best.hdf5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + LSTM  \n",
    "- checkpoint: try a pre-trained embedding layer e.g. GloVe Embedding, Word2Vec  \n",
    "  \n",
    "Building the embedding from scratch instead of using pre-trained (*this can be a slower approach, but tailors the model to a specific training dataset*)  \n",
    "\n",
    "Ref_1: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_checkpts = ModelCheckpoint(filepath=os.path.join(os.getcwd() + '/saved_models/lstm.weights.best.hdf5'), \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\syip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\syip\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20000, 128)        2560000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20000, 128)        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 19996, 64)         41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 4999, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 2,613,671\n",
      "Trainable params: 2,613,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features=20000\n",
    "embedding_size=128\n",
    "lstm_output_size=32  #70\n",
    "num_classes=7\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(input_dim=max_features, output_dim=embedding_size, input_length=20000))\n",
    "lstm.add(Dropout(0.25))\n",
    "lstm.add(Conv1D(filters=64,\n",
    "                 kernel_size=5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "lstm.add(MaxPooling1D(pool_size=4))\n",
    "lstm.add(LSTM(units=lstm_output_size))\n",
    "# lstm.add(Flatten())\n",
    "lstm.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "lstm.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5993 samples, validate on 3996 samples\n",
      "Epoch 1/3\n",
      "5993/5993 [==============================] - 1072s 179ms/step - loss: 1.5608 - acc: 0.4726 - val_loss: 1.5449 - val_acc: 0.4670\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.54495, saving model to C:\\Users\\syip\\Desktop\\DaSci\\Nanodegree\\ml_nanodegree\\capstone/saved_models/lstm.weights.best.hdf5\n",
      "Epoch 2/3\n",
      "5993/5993 [==============================] - 1074s 179ms/step - loss: 1.5429 - acc: 0.4746 - val_loss: 1.5357 - val_acc: 0.4670\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.54495 to 1.53567, saving model to C:\\Users\\syip\\Desktop\\DaSci\\Nanodegree\\ml_nanodegree\\capstone/saved_models/lstm.weights.best.hdf5\n",
      "Epoch 3/3\n",
      "5993/5993 [==============================] - 1067s 178ms/step - loss: 1.5419 - acc: 0.4746 - val_loss: 1.5396 - val_acc: 0.4670\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.53567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f206cfb550>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fit(x_train, y_train, validation_split=0.4, epochs=1, callbacks=[lstm_checkpts], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 2610})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lstm = lstm.predict_classes(x_test)\n",
    "Counter(y_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_accuracy = 100*np.sum(y_pred_lstm==y_true)/len(y_pred_lstm)\n",
    "print('Test accuracy: %.4f%%' % lstm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clstm = Sequential()\n",
    "# clstm.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3, input_shape=(n_features, 1),\n",
    "#                        border_mode='same', return_sequences=True))\n",
    "# clstm.add(BatchNormalization())\n",
    "# clstm.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n",
    "#                    border_mode='same', return_sequences=True))\n",
    "# clstm.add(BatchNormalization())\n",
    "# clstm.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n",
    "#                    border_mode='same', return_sequences=True))\n",
    "# clstm.add(BatchNormalization())\n",
    "# clstm.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n",
    "#                    border_mode='same', return_sequences=True))\n",
    "# clstm.add(BatchNormalization())\n",
    "# clstm.add(Convolution3D(nb_filter=1, kernel_dim1=1, kernel_dim2=3,\n",
    "#                       kernel_dim3=3, activation='sigmoid',\n",
    "#                       border_mode='same', dim_ordering='tf'))\n",
    "# clstm.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_lstm = load_model('./saved_models/lstm.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_lstm.predict_classes(x_test[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2610, 20000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2610, 20000, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6 samples, validate on 5 samples\n",
      "Epoch 1/1\n",
      "6/6 [==============================] - 3s 448ms/step - loss: 1.8966 - acc: 0.3333 - val_loss: 1.8113 - val_acc: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa0c44f9e8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fit(x_train[10:21], y_train[10:21], validation_split=0.4, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(lstm.predict_classes(x_test[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\syip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\syip\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 2,692,487\n",
      "Trainable params: 2,692,487\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features=20000\n",
    "embedding_size=128\n",
    "lstm_output_size=32  #70\n",
    "num_classes=7\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "          \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\syip\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/2\n",
      "100/100 [==============================] - 1004s 10s/step - loss: 1.9243 - acc: 0.3600 - val_loss: 1.8779 - val_acc: 0.4700\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 1072s 11s/step - loss: 1.8370 - acc: 0.5200 - val_loss: 1.7321 - val_acc: 0.4700\n",
      "2610/2610 [==============================] - 274s 105ms/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train[:100], y_train[:100],\n",
    "          epochs=2,\n",
    "          validation_data=(x_test[:100], y_true[:100]))\n",
    "score, acc = model.evaluate(x_test, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 2610})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lstm = model.predict_classes(x_test)\n",
    "Counter(y_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.0000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syip\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "lstm_accuracy = 100*np.sum(y_pred_lstm==y_true)/len(y_true)\n",
    "print('Test accuracy: %.4f%%' % lstm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec  \n",
    "Do transfer learning with pre-trained word embedding layers, such as Word2Vec & GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_tokens = [sentence.split() for sentence in x_dev]\n",
    "x_train_tokens = [sentence.split() for sentence in x_train]\n",
    "\n",
    "model = Word2Vec(\n",
    "    x_train_tokens,\n",
    "    size=150,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter passed to gensim.models.Word2Vec is an iterable of sentences. Sentences themselves are a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(x_train_tokens, total_examples=len(x_train), epochs=10)\n",
    "\n",
    "w = ['good']\n",
    "# w = filter(lambda x: x in model.vocab, x_train.tokens)\n",
    "model.wv.most_similar(positive=w,\n",
    "#                       topn=6\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract word embeddings from the Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('.saved_models/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))\n",
    "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
