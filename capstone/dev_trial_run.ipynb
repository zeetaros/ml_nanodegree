{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from factory_func import plot_confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, Lambda, LSTM, ConvLSTM2D, TimeDistributed, Masking, Bidirectional\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate, Activation, MaxPooling1D, GlobalAveragePooling1D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Records  \n",
    "SVM: 45.5172%  \n",
    "Dense NN: 44.0613%  \n",
    "CNN: 45.0192%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_raw = pd.read_csv(os.path.join(os.getcwd(), 'dev_sent_emo.csv'))\n",
    "train_raw = pd.read_csv(os.path.join(os.getcwd(), 'train_sent_emo.csv'))\n",
    "test_raw = pd.read_csv(os.path.join(os.getcwd(), 'test_sent_emo.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh my God, hes lost it. Hes totally lost it.</td>\n",
       "      <td>Phoebe</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:20:57,256</td>\n",
       "      <td>00:21:00,049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What?</td>\n",
       "      <td>Monica</td>\n",
       "      <td>surprise</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:21:01,927</td>\n",
       "      <td>00:21:03,261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Or! Or, we could go to the bank, close our acc...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:24,660</td>\n",
       "      <td>00:12:30,915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Youre a genius!</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:32,334</td>\n",
       "      <td>00:12:33,960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Aww, man, now we wont be bank buddies!</td>\n",
       "      <td>Joey</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:34,211</td>\n",
       "      <td>00:12:37,505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance   Speaker  \\\n",
       "0       1     Oh my God, hes lost it. Hes totally lost it.    Phoebe   \n",
       "1       2                                              What?    Monica   \n",
       "2       3  Or! Or, we could go to the bank, close our acc...      Ross   \n",
       "3       4                                   Youre a genius!  Chandler   \n",
       "4       5            Aww, man, now we wont be bank buddies!      Joey   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   sadness  negative            0             0       4        7   \n",
       "1  surprise  negative            0             1       4        7   \n",
       "2   neutral   neutral            1             0       4        4   \n",
       "3       joy  positive            1             1       4        4   \n",
       "4   sadness  negative            1             2       4        4   \n",
       "\n",
       "      StartTime       EndTime  \n",
       "0  00:20:57,256  00:21:00,049  \n",
       "1  00:21:01,927  00:21:03,261  \n",
       "2  00:12:24,660  00:12:30,915  \n",
       "3  00:12:32,334  00:12:33,960  \n",
       "4  00:12:34,211  00:12:37,505  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_raw.Utterance = dev_raw.Utterance.apply(lambda x: re.sub('\\\\x92', \"'\", x))\n",
    "train_raw.Utterance = train_raw.Utterance.apply(lambda x: re.sub('\\\\x92', \"'\", x))\n",
    "test_raw.Utterance = test_raw.Utterance.apply(lambda x: re.sub('\\\\x92', \"'\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = dev_raw.Utterance\n",
    "y_dev = dev_raw.Emotion\n",
    "x_train = train_raw.Utterance\n",
    "y_train = train_raw.Emotion\n",
    "x_test = test_raw.Utterance\n",
    "y_test = test_raw.Emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1,3))\n",
    "tfidf.fit(x_train)\n",
    "x_dev_tf = tfidf.transform(x_dev)\n",
    "x_train_tf = tfidf.transform(x_train)\n",
    "x_test_tf = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(n_components=300)\n",
    "# x_dev_tr = svd.fit_transform(x_dev_tf)\n",
    "# x_train_tr = svd.fit_transform(x_train_tf)\n",
    "# x_test_tr = svd.fit_transform(x_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=10, kernel='linear', probability=True)\n",
    "# param_grid = { \n",
    "#     'C': [1,10,100], 'kernel': ['linear', 'rbf']\n",
    "# }\n",
    "# clf = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5)\n",
    "# clf.fit(x_train_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model by GridSearchCV**  \n",
    "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,  \n",
    "decision_function_shape='ovr', degree=3, gamma='auto_deprecated',  \n",
    "kernel='linear', max_iter=-1, probability=True, random_state=None,  \n",
    "shrinking=True, tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X=x_train_tf, y=y_train)\n",
    "y_pred_svm = svm.predict(x_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42923347861223476"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=y_pred_svm, y_true=y_test, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 45.5172%\n"
     ]
    }
   ],
   "source": [
    "svm_accuracy = 100*np.sum(y_pred_svm==y_test)/len(y_pred_svm)\n",
    "print('Test accuracy: %.4f%%' % svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras TF-IDF tokenizer + Neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network won't accept sentences with different dimension(i.e. number of words) as input. By padding the inputs, we decide the maximum length of words in a sentence, then zero pads the rest, if the input length is shorter than the designated length. In the case where it exceeds the maximum length, then it will also truncate either from the beginning or from the end.  \n",
    "*Ref_1* https://keras.io/preprocessing/sequence/  \n",
    "*Ref_2* https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74\n",
    "\n",
    "https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_39 (Dense)             (None, 32)                640032    \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 651,367\n",
      "Trainable params: 651,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(set(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=20000))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D is generally good for text, whereas Conv2D is good for audio and images where spatial matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary_size = 20000\n",
    "# tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "# tokenizer.fit_on_texts(x_train)\n",
    "# sequences = tokenizer.texts_to_sequences(x_train)\n",
    "# data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = dev_raw.Utterance\n",
    "y_dev = dev_raw.Emotion\n",
    "x_train = train_raw.Utterance\n",
    "y_train = train_raw.Emotion\n",
    "x_test = test_raw.Utterance\n",
    "y_test = test_raw.Emotion\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_matrix(x_train, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(x_test, mode='tfidf')\n",
    "\n",
    "targets, uniques = pd.factorize(y_train, sort=True)\n",
    "y_train = to_categorical(targets, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.fit_on_sequences(x_dev)\n",
    "# tokenizer.texts_to_sequences(x_dev)\n",
    "# tokenizer.texts_to_matrix(x_dev, mode='tfidf')\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5993 samples, validate on 3996 samples\n",
      "Epoch 1/10\n",
      "5993/5993 [==============================] - 5s 895us/step - loss: 1.1143 - acc: 0.7215 - val_loss: 2.3272 - val_acc: 0.4467\n",
      "Epoch 2/10\n",
      "5993/5993 [==============================] - 4s 738us/step - loss: 0.3937 - acc: 0.8703 - val_loss: 2.6101 - val_acc: 0.4492\n",
      "Epoch 3/10\n",
      "5993/5993 [==============================] - 3s 582us/step - loss: 0.2599 - acc: 0.9137 - val_loss: 2.9333 - val_acc: 0.4399\n",
      "Epoch 4/10\n",
      "5993/5993 [==============================] - 3s 576us/step - loss: 0.2155 - acc: 0.9286 - val_loss: 3.2208 - val_acc: 0.4462\n",
      "Epoch 5/10\n",
      "5993/5993 [==============================] - 3s 561us/step - loss: 0.1933 - acc: 0.9338 - val_loss: 3.4009 - val_acc: 0.4367\n",
      "Epoch 6/10\n",
      "5993/5993 [==============================] - 3s 556us/step - loss: 0.1854 - acc: 0.9359 - val_loss: 3.5066 - val_acc: 0.4452\n",
      "Epoch 7/10\n",
      "5993/5993 [==============================] - 3s 560us/step - loss: 0.1758 - acc: 0.9384 - val_loss: 3.6438 - val_acc: 0.4424\n",
      "Epoch 8/10\n",
      "5993/5993 [==============================] - 3s 556us/step - loss: 0.1705 - acc: 0.9371 - val_loss: 3.7864 - val_acc: 0.4520\n",
      "Epoch 9/10\n",
      "5993/5993 [==============================] - 3s 554us/step - loss: 0.1674 - acc: 0.9384 - val_loss: 3.7963 - val_acc: 0.4512\n",
      "Epoch 10/10\n",
      "5993/5993 [==============================] - 4s 587us/step - loss: 0.1582 - acc: 0.9419 - val_loss: 3.8793 - val_acc: 0.4402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b63ad98668>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.4, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 0, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_data = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=50)\n",
    "model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 1578, 3: 324, 0: 261, 5: 145, 6: 232, 2: 44, 1: 26})"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([np.argmax(model.predict(np.expand_dims(seq, axis=0))) for seq in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 4710, 6: 1205, 2: 268, 5: 683, 3: 1743, 1: 271, 0: 1109})"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = dict(zip(list(uniques), range(num_classes)))\n",
    "label_map\n",
    "y_true = list(map(label_map.get, y_test))\n",
    "len(y_true) == len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 44.0613%\n"
     ]
    }
   ],
   "source": [
    "# y_pred = [np.argmax(model.predict(np.expand_dims(seq, axis=0))) for seq in x_test]\n",
    "y_pred = model.predict_classes(x_test)\n",
    "test_accuracy = 100*np.sum(y_pred==y_true)/len(y_pred)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(np.array([1, 3, 2, 0, 3, 2, 2, 1, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9989, 20000)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'], dtype='object')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "n_length = x_train.shape[0]\n",
    "n_features = x_train.shape[1]\n",
    "\n",
    "x_train_reshaped = x_train.reshape(n_length, n_features, 1)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], n_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2610, 20000)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape\n",
    "# x_train_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_38 (Conv1D)           (None, 20000, 16)         48        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 10000, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 10000, 32)         1056      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 5000, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 5000, 64)          4160      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 2500, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160000)            0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 128)               20480128  \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 20,486,295\n",
      "Trainable params: 20,486,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv1D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(n_features, 1)))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "cnn.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "cnn.add(Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn.add(MaxPooling1D(pool_size=2))\n",
    "# cnn.add(GlobalAveragePooling1D())\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "cnn.add(Dense(num_classes, activation='softmax'))\n",
    "cnn.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5993 samples, validate on 3996 samples\n",
      "Epoch 1/5\n",
      "5993/5993 [==============================] - 193s 32ms/step - loss: 1.5201 - acc: 0.4774 - val_loss: 1.4750 - val_acc: 0.4905\n",
      "Epoch 2/5\n",
      "5993/5993 [==============================] - 192s 32ms/step - loss: 1.1800 - acc: 0.5653 - val_loss: 1.5217 - val_acc: 0.4757\n",
      "Epoch 3/5\n",
      "5993/5993 [==============================] - 179s 30ms/step - loss: 0.8135 - acc: 0.7002 - val_loss: 1.7836 - val_acc: 0.4677\n",
      "Epoch 4/5\n",
      "5993/5993 [==============================] - 170s 28ms/step - loss: 0.5779 - acc: 0.7903 - val_loss: 2.2404 - val_acc: 0.4710\n",
      "Epoch 5/5\n",
      "5993/5993 [==============================] - 168s 28ms/step - loss: 0.4314 - acc: 0.8443 - val_loss: 2.5079 - val_acc: 0.4532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b646058080>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x_train_reshaped, y_train, validation_split=0.4, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnn = cnn.predict_classes(x_test_reshaped)\n",
    "# y_pred_cnn = [np.argmax(x) for x in cnn.predict(x_test_reshaped)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4: 1557, 3: 352, 5: 153, 6: 317, 0: 173, 1: 33, 2: 25})"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_pred_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 45.0192%\n"
     ]
    }
   ],
   "source": [
    "cnn_accuracy = 100*np.sum(y_pred_cnn==y_true)/len(y_pred_cnn)\n",
    "print('Test accuracy: %.4f%%' % cnn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec + LSTM  \n",
    "- checkpoint: try a pre-trained embedding layer e.g. GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 96, 64)            41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 70)                37800     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 497       \n",
      "=================================================================\n",
      "Total params: 2,639,321\n",
      "Trainable params: 2,639,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features=20000\n",
    "embedding_size=128\n",
    "lstm_output_size=70\n",
    "num_classes=7\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(input_dim=max_features, output_dim=embedding_size, input_length=100))\n",
    "lstm.add(Dropout(0.25))\n",
    "lstm.add(Conv1D(filters=64,\n",
    "                 kernel_size=5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "lstm.add(MaxPooling1D(pool_size=4))\n",
    "lstm.add(LSTM(units=lstm_output_size))\n",
    "lstm.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "lstm.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clstm = Sequential()\n",
    "# clstm.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3, input_shape=(n_features, 1),\n",
    "#                        border_mode='same', return_sequences=True))\n",
    "# clstm.add(BatchNormalization())\n",
    "# clstm.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n",
    "#                    border_mode='same', return_sequences=True))\n",
    "# clstm.add(BatchNormalization())\n",
    "# clstm.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n",
    "#                    border_mode='same', return_sequences=True))\n",
    "# clstm.add(BatchNormalization())\n",
    "# clstm.add(ConvLSTM2D(nb_filter=40, nb_row=3, nb_col=3,\n",
    "#                    border_mode='same', return_sequences=True))\n",
    "# clstm.add(BatchNormalization())\n",
    "# clstm.add(Convolution3D(nb_filter=1, kernel_dim1=1, kernel_dim2=3,\n",
    "#                       kernel_dim3=3, activation='sigmoid',\n",
    "#                       border_mode='same', dim_ordering='tf'))\n",
    "# clstm.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_tokens = [sentence.split() for sentence in x_dev]\n",
    "x_train_tokens = [sentence.split() for sentence in x_train]\n",
    "\n",
    "model = Word2Vec(\n",
    "    x_train_tokens,\n",
    "    size=150,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter passed to gensim.models.Word2Vec is an iterable of sentences. Sentences themselves are a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(x_train_tokens, total_examples=len(x_train), epochs=10)\n",
    "\n",
    "w = ['good']\n",
    "# w = filter(lambda x: x in model.vocab, x_train.tokens)\n",
    "model.wv.most_similar(positive=w,\n",
    "#                       topn=6\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
