{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spectrum\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import getenv\n",
    "from glob import glob\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from librosa.feature import chroma_stft\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sentsation.connect import get_s3_client, get_s3_resource\n",
    "from sentsation.audio.feature_extraction import AudioFeature\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert files to .wav  \n",
    "either get local files or from AWS S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3_KEY = getenv('AWS_KEY')\n",
    "# S3_SECRET = getenv('AWS_SECRET')\n",
    "# AWS_REGION = getenv('AWS_REGION', 'eu-west-2')\n",
    "# bucket_name = getenv('S3_BUCKET')\n",
    "# s3r = get_s3_resource(session_token)\n",
    "# bucket = s3r.Bucket(bucket_name)\n",
    "# s3 = get_s3_client(session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Explore various features that can be extracted from an audio file and select the most representative ones for model training  \n",
    "\n",
    "Ref: https://librosa.github.io/librosa/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia0_utt0.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic: sample rate, duration\n",
    "Loads and decodes the audio as a time series y, represented as a one-dimensional NumPy floating point array. The variable sr contains the sampling rate of y, that is, the number of samples per second of audio. By default, all audio is mixed to mono and resampled to 22050 Hz at load time. This behavior can be overridden by supplying additional arguments to librosa.load().  \n",
    "- Load the audio as a waveform *y* [audio signal]  \n",
    "- store the sampling rate as *sample_rate* [audio sampling rate of y]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sample_rate = librosa.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_rate, len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.get_duration(y, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "librosa.display.waveplot(y, sr=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the default beat tracker. The output of the beat tracker is an estimate of the tempo (in beats per minute), and an array of frame numbers corresponding to detected beat events.\n",
    "\n",
    "Frames here correspond to short windows of the signal (y), each separated by hop_length = 512 samples. Since v0.3, librosa uses centered frames, so that the kth frame is centered around sample k * hop_length.\n",
    "\n",
    "The next operation converts the frame numbers beat_frames into timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated tempo: {:.2f} beats per minute'.format(tempo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the frame indices of beat events into timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_times = librosa.frames_to_time(beat_frames, sr=sample_rate)\n",
    "beat_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC  \n",
    "Mel Frequency Cepstral Coefficents (MFCCs) are a feature widely used in automatic speech and speaker recognition.  \n",
    "Compute MFCC features from the raw signal  \n",
    "- *y* [audio time series]\n",
    "- *n_mfcc* [number of MFCCs to return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=y, sr=sample_rate, hop_length=256, n_mfcc=14)\n",
    "print('Computed {} MFCCs over {} frames.'.format(mfccs.shape[0], mfccs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_avg = np.mean(mfccs.T,axis=0) \n",
    "mfcc_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "librosa.display.specshow(mfccs, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Scaling__  \n",
    "Scale the MFCCs such that each coefficient dimension has zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = scale(mfccs, axis=1)\n",
    "print('Mean: \\n', mfccs.mean(axis=1), '\\n\\nVariance: \\n', mfccs.var(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power / Energy\n",
    "Apply Short-time Fourier transform (stft)  \n",
    "- np.abs(D[f, t]) is the magnitude of frequency bin f at frame t  \n",
    "- np.angle(D[f, t]) is the phase of frequency bin f at frame t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.abs(librosa.stft(y))\n",
    "# Compute a chromagram from a waveform or power spectrogram\n",
    "chroma = chroma_stft(S=S, sr=sample_rate)\n",
    "print('Chroma is a array with shape: ', chroma.shape)\n",
    "chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), y_axis='log', x_axis='time')\n",
    "plt.title('Power spectrogram')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy  \n",
    "Computing the RMS value from audio samples is faster as it doesn’t require a STFT calculation. However, using a spectrogram will give a more accurate representation of energy over time because its frames can be windowed, thus prefer using S if it’s already available.  \n",
    "- *S* [spectrogram]  \n",
    "\n",
    "Example: https://musicinformationretrieval.com/energy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.magphase(librosa.stft(y, window=np.ones, center=False))[0]\n",
    "librosa.feature.rms(S=S)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.feature.rms(y=y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the short-time energy using a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = 256\n",
    "frame_length = 512\n",
    "ST_energy = np.array([\n",
    "    sum(abs(y[i:i+frame_length]**2))\n",
    "    for i in range(0, len(y), hop_length)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(librosa.feature.rms(S=S)[0]), len(librosa.feature.rms(y=y)[0]), len(ST_energy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_rmse = librosa.feature.rmse(y, frame_length=frame_length, hop_length=hop_length, center=True)\n",
    "len(ST_rmse[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = range(len(ST_energy))\n",
    "time = librosa.frames_to_time(frames, sr=sample_rate, hop_length=hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "librosa.display.waveplot(y, sr=sample_rate, alpha=0.4, )\n",
    "plt.plot(time, ST_energy/ST_energy.max(), 'r--')             # normalized for visualization\n",
    "plt.plot(time, (ST_rmse/ST_rmse.max())[0], color='g')              # normalized for visualization\n",
    "plt.legend(('Energy', 'RMSE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitch  \n",
    "Pitch is one of the characteristics of a speech signal and is measured as the frequency of the signal  \n",
    "- \"pitch_tuning\": Given a collection of pitches, estimate its tuning offset (in fractions of a bin) relative to A440=440.0Hz.  \n",
    "- \"piptrack\": Pitch tracking on thresholded parabolically-interpolated STFT.  \n",
    "    - *threshold* - A bin in spectrum X is considered a pitch when it is greater than threshold*X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitches, magnitudes = librosa.piptrack(y, sample_rate, threshold=0)\n",
    "# Select out pitches with high energy\n",
    "# pitches = pitches[magnitudes > np.median(magnitudes)]\n",
    "librosa.pitch_tuning(pitches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Comparison] Characteristics of different sentiment / emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_sadness = '/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia0_utt0.wav'\n",
    "af_surprise = '/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia0_utt1.wav'\n",
    "af_neutral = '/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia1_utt0.wav'\n",
    "af_joy = '/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia1_utt1.wav'\n",
    "af_anger = '/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia1_utt11.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sadness, sr_sadness = librosa.load(af_sadness)\n",
    "y_surprise, sr_surprise = librosa.load(af_surprise)\n",
    "y_neutral, sr_neutral= librosa.load(af_neutral)\n",
    "y_joy, sr_joy = librosa.load(af_joy)\n",
    "y_anger, sr_anger = librosa.load(af_anger)\n",
    "\n",
    "y_list = [y_sadness, y_surprise, y_neutral, y_joy, y_anger]\n",
    "sr_list = [sr_sadness, sr_surprise, sr_neutral, sr_joy, sr_anger]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mfcc(y, sr, hop_length=None, n_mfcc=None, use_scale=False, label=None):\n",
    "    hop_length = hop_length or 256\n",
    "    n_mfcc = n_mfcc or 13\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sample_rate, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "    if scale:\n",
    "        mfccs = scale(mfccs, axis=1)\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    librosa.display.specshow(mfccs, x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC for {}'.format(label))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y, sr, label in zip(y_list, sr_list, ['sadness','surprise','neutral','joy','anger']):\n",
    "    plot_mfcc(y, sr, use_scale=True, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short-term Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_energy(y, sr, hop_length=None, frame_length=None, label=None):\n",
    "    hop_length = hop_length or 256\n",
    "    frame_length = frame_length or 512\n",
    "    \n",
    "    energy = np.array([sum(abs(y[i:i+frame_length]**2)) for i in range(0, len(y), hop_length)])\n",
    "    rmse = librosa.feature.rmse(y, frame_length=frame_length, hop_length=hop_length, center=True)\n",
    "    frames = range(len(energy))\n",
    "    time = librosa.frames_to_time(frames, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    librosa.display.waveplot(y, sr=sr, alpha=0.4, )\n",
    "    plt.plot(time, energy/energy.max(), 'r--')\n",
    "    plt.plot(time, (rmse/rmse.max())[0], color='g')\n",
    "    plt.title('Energy for {}'.format(label))\n",
    "    plt.legend(('Energy', 'RMSE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y, sr, label in zip(y_list, sr_list, ['sadness','surprise','neutral','joy','anger']):\n",
    "    plot_energy(y, sr, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction  \n",
    "Kick off the feature extraction process for a list of audio files with selected features from previous section  \n",
    "Firstly, save an image of mfcc / power spectrogram for each audio file  \n",
    "(think about mfcc as reducing dimension of the power spectrogram, like applying TruncatedSVD) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1112 audiofiles from dev; \n",
      "Loaded 9988 audiofiles from train; \n",
      "Loaded 2747 audiofiles from test\n"
     ]
    }
   ],
   "source": [
    "path_to_audiofiles = '/Users/cheuky/Downloads/MELD.RAW/audio'\n",
    "afs_dev = glob(os.path.join(path_to_audiofiles,'dev','*.wav'))\n",
    "afs_train = glob(os.path.join(path_to_audiofiles,'train','*.wav'))\n",
    "afs_test = glob(os.path.join(path_to_audiofiles,'test','*.wav'))\n",
    "print('Loaded',len(afs_dev),'audiofiles from dev; \\nLoaded',len(afs_train),'audiofiles from train;',\n",
    "    '\\nLoaded',len(afs_test),'audiofiles from test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [af.split('/')[-1] for af in afs_dev]\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '.'.join(filenames[0].split('.')[:-1].append('jpg'))\n",
    "filenames[0].split('.')[:-1] + ['jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc(audiofilelist, save_dir, hop_length=None, n_mfcc=None, use_scale=False):\n",
    "    hop_length = hop_length or 256\n",
    "    n_mfcc = n_mfcc or 13\n",
    "    for af in audiofilelist:\n",
    "        y, sr = librosa.load(af)\n",
    "        filename = str(af.split('/')[-1])\n",
    "        imagename = '.'.join(filename.split('.')[:-1] + ['jpg'])\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "        if scale:\n",
    "            mfccs = scale(mfccs, axis=1)\n",
    "        plt.figure(figsize=(25.6, 25.6), dpi=10)\n",
    "        librosa.display.specshow(mfccs)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('/'.join([save_dir, imagename]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afs_trial = glob(os.path.join(path_to_audiofiles,'trial','*.wav'))\n",
    "afext = AudioFeature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afext.save_mfcc(afs_dev, '/Users/cheuky/Downloads/MELD.RAW/features/mfcc/dev')\n",
    "# save_mfcc(afs_trial, '/Users/cheuky/Downloads/MELD.RAW/features/mfcc/dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Metadata  \n",
    "Obtain the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1109 datapoints from dev; \n",
      "Loaded 9989 datapoints from train; \n",
      "Loaded 2610 datapoints from test.\n"
     ]
    }
   ],
   "source": [
    "path_to_metadata = '/Users/cheuky/Downloads/MELD.RAW'\n",
    "meta_dev = pd.read_csv(os.path.join(path_to_metadata, 'dev_sent_emo.csv'))\n",
    "meta_train = pd.read_csv(os.path.join(path_to_metadata, 'train_sent_emo.csv'))\n",
    "meta_test = pd.read_csv(os.path.join(path_to_metadata, 'test_sent_emo.csv'))\n",
    "print(\"Loaded {} datapoints from dev; \\nLoaded {} datapoints from train; \\nLoaded {} datapoints from test.\".format(meta_dev.shape[0],\n",
    "                                                                             meta_train.shape[0],\n",
    "                                                                             meta_test.shape[0]))\n",
    "meta_dev['Source'] = 'dev'\n",
    "meta_train['Source'] = 'train'\n",
    "meta_test['Source'] = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain a list of audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1112 audiofiles from dev; \n",
      "Loaded 9988 audiofiles from train; \n",
      "Loaded 2747 audiofiles from test\n"
     ]
    }
   ],
   "source": [
    "print('Loaded',len(afs_dev),'audiofiles from dev; \\nLoaded',len(afs_train),'audiofiles from train;',\n",
    "    '\\nLoaded',len(afs_test),'audiofiles from test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_master = pd.concat([meta_dev, meta_train, meta_test])\n",
    "meta_master['Filename'] = meta_master.apply(lambda row: \"\".join(['dia', str(row['Dialogue_ID']), '_',\n",
    "                                                                'utt', str(row['Utterance_ID']), '.wav']) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with shape:  (13708, 13)\n"
     ]
    }
   ],
   "source": [
    "print('Dataframe with shape: ', meta_master.shape)\n",
    "meta_master['Filepath']=meta_master.apply(lambda r: '/'.join([path_to_audiofiles, r['Source'], r['Filename']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join target labels with audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "afs_df_dev = pd.DataFrame(afs_dev, columns=['Filepath'])\n",
    "afs_df_train = pd.DataFrame(afs_train, columns=['Filepath'])\n",
    "afs_df_test = pd.DataFrame(afs_test, columns=['Filepath'])\n",
    "# afs_df_master = afs_df_dev.append([afs_df_train, afs_df_test])\n",
    "afs_df_master = pd.concat([afs_df_dev, afs_df_train, afs_df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13708 metadata points and 13847 audio files. A difference of 139.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} metadata points and {} audio files. A difference of {}.'.format(\n",
    "    meta_master.shape[0], afs_df_master.shape[0], abs(meta_master.shape[0]-afs_df_master.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_master = pd.merge(meta_master, afs_df_master, how='inner', on='Filepath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe with shape:  (13706, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>Source</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh my God, hes lost it. Hes totally lost it.</td>\n",
       "      <td>Phoebe</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:20:57,256</td>\n",
       "      <td>00:21:00,049</td>\n",
       "      <td>dev</td>\n",
       "      <td>dia0_utt0.wav</td>\n",
       "      <td>/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What?</td>\n",
       "      <td>Monica</td>\n",
       "      <td>surprise</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:21:01,927</td>\n",
       "      <td>00:21:03,261</td>\n",
       "      <td>dev</td>\n",
       "      <td>dia0_utt1.wav</td>\n",
       "      <td>/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Or! Or, we could go to the bank, close our acc...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:24,660</td>\n",
       "      <td>00:12:30,915</td>\n",
       "      <td>dev</td>\n",
       "      <td>dia1_utt0.wav</td>\n",
       "      <td>/Users/cheuky/Downloads/MELD.RAW/audio/dev/dia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance Speaker  \\\n",
       "0       1     Oh my God, hes lost it. Hes totally lost it.  Phoebe   \n",
       "1       2                                              What?  Monica   \n",
       "2       3  Or! Or, we could go to the bank, close our acc...    Ross   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   sadness  negative            0             0       4        7   \n",
       "1  surprise  negative            0             1       4        7   \n",
       "2   neutral   neutral            1             0       4        4   \n",
       "\n",
       "      StartTime       EndTime Source       Filename  \\\n",
       "0  00:20:57,256  00:21:00,049    dev  dia0_utt0.wav   \n",
       "1  00:21:01,927  00:21:03,261    dev  dia0_utt1.wav   \n",
       "2  00:12:24,660  00:12:30,915    dev  dia1_utt0.wav   \n",
       "\n",
       "                                            Filepath  \n",
       "0  /Users/cheuky/Downloads/MELD.RAW/audio/dev/dia...  \n",
       "1  /Users/cheuky/Downloads/MELD.RAW/audio/dev/dia...  \n",
       "2  /Users/cheuky/Downloads/MELD.RAW/audio/dev/dia...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dataframe with shape: ', data_master.shape)\n",
    "data_master.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join features with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Source</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>dev</td>\n",
       "      <td>dia0_utt0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surprise</td>\n",
       "      <td>negative</td>\n",
       "      <td>dev</td>\n",
       "      <td>dia0_utt1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dev</td>\n",
       "      <td>dia1_utt0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>dev</td>\n",
       "      <td>dia1_utt1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>dev</td>\n",
       "      <td>dia1_utt2.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Emotion Sentiment Source       Filename\n",
       "0   sadness  negative    dev  dia0_utt0.wav\n",
       "1  surprise  negative    dev  dia0_utt1.wav\n",
       "2   neutral   neutral    dev  dia1_utt0.wav\n",
       "3       joy  positive    dev  dia1_utt1.wav\n",
       "4   sadness  negative    dev  dia1_utt2.wav"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_mfcc = '/Users/cheuky/Downloads/MELD.RAW/features/mfcc'\n",
    "mfcc_df = data_master[['Emotion','Sentiment','Source','Filename']]\n",
    "mfcc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheuky/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mfcc_df['MFCCpath'] = mfcc_df.apply(lambda r: '/'.join([path_to_mfcc, r.Source,\n",
    "                                              r.Filename.replace('.wav','.jpg')]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/cheuky/Downloads/MELD.RAW/features/mfcc/dev/dia0_utt0.jpg',\n",
       " '/Users/cheuky/Downloads/MELD.RAW/features/mfcc/dev/dia0_utt1.jpg',\n",
       " '/Users/cheuky/Downloads/MELD.RAW/features/mfcc/dev/dia1_utt0.jpg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_df.head(3)\n",
    "list(mfcc_df.MFCCpath[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))  # default input size for ResNet50 is 224x224\n",
    "    x = image.img_to_array(img)  # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm_notebook(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(mfcc_df[mfcc_df.Source=='dev'].MFCCpath)\n",
    "targets, uniques = pd.factorize(mfcc_df[mfcc_df.Source=='dev'].Sentiment)\n",
    "y = to_categorical(targets, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ecbe359a3d493c99562e8a6504b1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1108), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
    "\n",
    "X = paths_to_tensor(features).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = numpy.vstack((mfcc_brahms_scaled, mfcc_busta_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN  \n",
    "With the MFCC graphs, the audio-classification problem is now transformed into an image classification problem. Just like detecting presence of a particular entity (‘Dog’,’Cat’,’Car’ etc) in images.  \n",
    "Consider using a CNN to classify the spectrogram images, because CNN works better in detecting local feature patterns (edges etc) in different parts of the image and are also good at capturing hierarchical features which become subsequently complex with every layer as illustrated in the image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 4,595\n",
      "Trainable params: 4,595\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "cnn.add(MaxPooling2D(pool_size=2))\n",
    "cnn.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "cnn.add(MaxPooling2D(pool_size=2))\n",
    "cnn.add(GlobalAveragePooling2D())\n",
    "# cnn.add(Flatten())\n",
    "cnn.add(Dense(64, activation='relu'))\n",
    "cnn.add(Dense(3, activation='softmax'))\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "742/742 [==============================] - 12s 16ms/step - loss: 1.0754 - acc: 0.3922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137fe7b00>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(X_train, y_train, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([np.argmax(cnn.predict(np.expand_dims(tensor, axis=0))) for tensor in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnnhistory.history['acc'])\n",
    "plt.plot(cnnhistory.history['val_acc'])\n",
    "plt.title('CNN accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train','test'], loc='upper_left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# feature = mfccs\n",
    "# label = row.Class\n",
    "# X = np.array(temp.feature.tolist())\n",
    "# y = np.array(temp.label.tolist())\n",
    "\n",
    "# lb = LabelEncoder()\n",
    "\n",
    "# y = np_utils.to_categorical(lb.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "220px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
